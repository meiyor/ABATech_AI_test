{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fc1495",
   "metadata": {},
   "outputs": [],
   "source": [
    "## loading libraries and Python, Tensorflow and Keras packages\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA, SparsePCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.layers import ReLU\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam, SGD\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import load_model\n",
    "from matplotlib import pyplot\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow import keras\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af7388e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs=124\n",
    "weight_decay = 0.0001\n",
    "\n",
    "data=[]\n",
    "## read csv file\n",
    "dataframeObject = pd.DataFrame(pd.read_csv(str(sys.argv[1])))\n",
    "\n",
    "## assigning the features names and putting them in a list\n",
    "features=list(dataframeObject.columns.values)\n",
    "\n",
    "for index in range(1,len(features)):\n",
    "    index_feature=features[index]\n",
    "    dataframeObject[[index_feature]].replace(np.nan,0)\n",
    "    data.append(dataframeObject[[index_feature]].to_numpy())\n",
    "\n",
    "data=np.squeeze(np.array(data))\n",
    "shape=np.shape(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed1b94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert all strings in integer values\n",
    "for count in range(0,shape[0]):\n",
    "  possibilities=[]\n",
    "  data_temp=[]\n",
    "  if isinstance(data[count,0],str):\n",
    "    for in_count in range(0,shape[1]):\n",
    "       if not(data[count,in_count] in possibilities):\n",
    "          possibilities.append(data[count,in_count])\n",
    "       index_val = int(possibilities.index(data[count,in_count]))\n",
    "       data_temp.append(index_val)\n",
    "    data[count,:]=np.array(data_temp)\n",
    "\n",
    "shape=np.shape(data)\n",
    "#print(data,shape,'data_after')\n",
    "\n",
    "## data definition\n",
    "DATA=np.transpose(data[0:22,:])\n",
    "labels=data[22,:]\n",
    "\n",
    "shape=np.shape(DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafa1532",
   "metadata": {},
   "outputs": [],
   "source": [
    "## crossvalidation definition\n",
    "kf = KFold(n_splits=int(sys.argv[2]))\n",
    "kf.get_n_splits(DATA)\n",
    "acc=np.zeros([int(sys.argv[2])])\n",
    "\n",
    "KFold(n_splits=int(sys.argv[2]), random_state=None, shuffle=False) ## define the folding parameter in the input\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(DATA)):\n",
    "        tf.random.set_seed(1234)\n",
    "        np.random.seed(1234)\n",
    "        random.seed(1234)\n",
    "        print(f\":Fold {i}:\")\n",
    "        #print(f\"  Train: index={train_index}\")\n",
    "        #print(f\"  Test:  index={test_index}\")\n",
    "        transforms = list()\n",
    "        transforms.append(('mms', MinMaxScaler()))\n",
    "        transforms.append(('ss', StandardScaler()))\n",
    "        transforms.append(('rs', RobustScaler()))\n",
    "        transforms.append(('qt', QuantileTransformer(n_quantiles=100, output_distribution='normal')))\n",
    "        transforms.append(('kbd', KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform')))\n",
    "        transforms.append(('pca', PCA(n_components=7)))\n",
    "        #transforms.append(('sparsepca', SparsePCA(n_components=7)))\n",
    "        transforms.append(('svd', TruncatedSVD(n_components=7)))\n",
    "        ## decoder definition for each fold\n",
    "        visible = Input(shape=(n_inputs,))\n",
    "        ## decoder level 1\n",
    "        e = Dense(100,kernel_initializer=tf.keras.initializers.GlorotNormal())(visible)\n",
    "        #e = BatchNormalization()(e)\n",
    "        e = ReLU()(e)\n",
    "        #e = Dropout(0.7)(e)\n",
    "        e = Dense(10,kernel_initializer=tf.keras.initializers.GlorotNormal())(e)\n",
    "        #e = BatchNormalization()(e)\n",
    "        e = ReLU()(e)\n",
    "        #e = Dropout(0.2)(e)\n",
    "        # output layer\n",
    "        output = Dense(2, activation='softmax')(e)\n",
    "        # create the feature union\n",
    "        fu = FeatureUnion(transforms)\n",
    "         # define the model\n",
    "        deep = Model(inputs=visible, outputs=output)\n",
    "        # compile decoder model\n",
    "        lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(0.001,decay_steps=20,decay_rate=0.1)\n",
    "        opt = keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "        deep.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        DATA_train=DATA[train_index,:]\n",
    "        DATA_test=DATA[test_index,:]\n",
    "        ## add the normalizer in the steps\n",
    "        steps = list()\n",
    "        steps.append(('fu', fu))\n",
    "        scaler=MinMaxScaler()\n",
    "        steps.append(('sc',scaler))\n",
    "        pipeline = Pipeline(steps=steps)\n",
    "        DATA_train_encoder = pipeline.fit_transform(DATA_train,labels[train_index].astype('int'))\n",
    "        DATA_test_encoder = pipeline.transform(DATA_test)\n",
    "        \n",
    "        ## fit the model and check the accuracies\n",
    "        history = deep.fit(DATA_train_encoder.astype(float),tf.keras.utils.to_categorical(labels[train_index].astype('int'),num_classes=2), epochs=350, batch_size=200, verbose=2,  validation_data=(DATA_test_encoder.astype(float),tf.keras.utils.to_categorical(labels[test_index].astype('int'),num_classes=2)))\n",
    "        ## plot if the user wants\n",
    "        if int(sys.argv[3])==1:\n",
    "            pyplot.plot(history.history['loss'], label='train')\n",
    "            pyplot.plot(history.history['val_loss'], label='test')\n",
    "            pyplot.legend()\n",
    "            pyplot.show()\n",
    "        labels_predict_train = deep.predict(DATA_train_encoder.astype(np.float32))\n",
    "        # decode the test data\n",
    "        labels_predict_test = deep.predict(DATA_test_encoder.astype(np.float32))\n",
    "        labels_train=np.argmax(labels_predict_train, axis=1)\n",
    "        labels_test=np.argmax(labels_predict_test, axis=1)\n",
    "        labels_train=np.squeeze(labels_train)\n",
    "        labels_test=np.squeeze(labels_test)\n",
    "        # calculate classification accuracy\n",
    "        acc[i] = accuracy_score(labels[test_index].astype('int'), labels_test)\n",
    "        print(acc[i])\n",
    "        ### reset the model weights and the tf graph session as well as deleting the current model\n",
    "        keras.backend.clear_session()\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "        del deep\n",
    "        gc.collect()\n",
    "##print the final accuracy        \n",
    "acc_mean=np.mean(acc)\n",
    "acc_std=np.std(acc)\n",
    "print(f\":accuracy:{acc_mean} +/- {acc_std}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
